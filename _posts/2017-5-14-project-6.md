
# Movie Ratings Project

In this project, I used classification modeling to examine what factors lead to certain ratings for movies, based on a dataset I collected from IMDB (www.imdb.com).


### Assumption

Before we go to the results, here are two major assumptions I made for this project.
1. Data collection: data was collected from two parts
    - Top 250 movies on IMDB
    - The 50 most popular movies by genre (action, adventure, animation, biography, comedy, crime, documentary, drama, family, fantasy, history, horror, musical, mystery, romance, sport, thriller, war, western).

2. Labeling: imdb rating is my dependent variable. The lowest imdb rating for the top 250 movies was 8.0. I labeled
    - 1 for movies that had imdb rating equal or above 8.0
    - 0 for movies that had imdb rating below 8.0

3. Imputing: imputed means for NaN values in Box office and Metascore columns 


### Summary
I applied Random forest and Extra trees for the modeling section. Both models had the number of imdbvotes, movie year and Metascore in the top 5 features.

> A greater number of imdbvotes indicates more people have watched the movie. This can a factor to see if the movie is sucessful or not.

> Old movies are more likely to be rated higher. Since I collected the data from the best and the most popular movies, old movies that are still popular are usually classic and great ones. 

> Metascore has a relatively positive relationship with imdbrating. That means it contains some information for my dependent variable.


Random forests had a higher accuracy score on test dataset (0.78 vs 0.72)


### Project Outline
- Get data
- Clean data
- Add features
- Visualization
- Random Forest
- Extra trees

## Next Steps

In next steps, I would like to take a deeper look around my assumptions.

1. Data collection: get a list of randomly selected movies since 1900 and webscrape more data
2. Apply KNN imputer for NaN values
4. Examine outliners in visualizations
3. Try more models eg. baggings, boosting, logistic regression.


```python
import requests
import pandas as pd
from imdbpie import Imdb
import numpy as np
```

## Get data


```python
# Get top250 movies from imdbpie
imdb = Imdb()
imdb = Imdb(anonymize=True)
top_250 = imdb.top_250()
df_top_250 = pd.DataFrame(top_250)
```


```python
df_top_250.head()
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>can_rate</th>
      <th>image</th>
      <th>num_votes</th>
      <th>rating</th>
      <th>tconst</th>
      <th>title</th>
      <th>type</th>
      <th>year</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>True</td>
      <td>{u'url': u'https://images-na.ssl-images-amazon...</td>
      <td>1808376</td>
      <td>9.3</td>
      <td>tt0111161</td>
      <td>The Shawshank Redemption</td>
      <td>feature</td>
      <td>1994</td>
    </tr>
    <tr>
      <th>1</th>
      <td>True</td>
      <td>{u'url': u'https://images-na.ssl-images-amazon...</td>
      <td>1234863</td>
      <td>9.2</td>
      <td>tt0068646</td>
      <td>The Godfather</td>
      <td>feature</td>
      <td>1972</td>
    </tr>
    <tr>
      <th>2</th>
      <td>True</td>
      <td>{u'url': u'https://images-na.ssl-images-amazon...</td>
      <td>849748</td>
      <td>9.0</td>
      <td>tt0071562</td>
      <td>The Godfather: Part II</td>
      <td>feature</td>
      <td>1974</td>
    </tr>
    <tr>
      <th>3</th>
      <td>True</td>
      <td>{u'url': u'https://images-na.ssl-images-amazon...</td>
      <td>1788444</td>
      <td>9.0</td>
      <td>tt0468569</td>
      <td>The Dark Knight</td>
      <td>feature</td>
      <td>2008</td>
    </tr>
    <tr>
      <th>4</th>
      <td>True</td>
      <td>{u'url': u'https://images-na.ssl-images-amazon...</td>
      <td>489687</td>
      <td>8.9</td>
      <td>tt0050083</td>
      <td>12 Angry Men</td>
      <td>feature</td>
      <td>1957</td>
    </tr>
  </tbody>
</table>
</div>




```python
id_list_1 = df_top_250['tconst'].tolist()
```


```python
# Get 50 popular movies for each genre
import requests
from bs4 import BeautifulSoup
genre = ['action','adventure','animation','biography','comedy','crime', 'documentary','drama','family', 'fantasy'\
         ,'history', 'horror', 'musical', 'mystery', 'romance','sport','thriller','war', 'western']
id_list_2 = []

for i in genre:
    url = 'http://www.imdb.com/search/title?genres={}&title_type=feature&sort=moviemeter,asc&page=1&ref_=adv_nxt'.format(i)
    r = requests.get(url)
    soup = BeautifulSoup(r.content,'lxml')
    div = soup.findAll('div',class_='ribbonize')
    for k in div:
        id_number = k.get('data-tconst')
        id_list_2.append(id_number)
```


```python
len(id_list_2)
```




    900




```python
id_list = id_list_1 + id_list_2
```


```python
len(id_list)
```




    1150




```python
# Search imdb ID on omdbapi

df_origin = pd.DataFrame()

for i in id_list:
    r = requests.get('http://www.omdbapi.com/?i={}&plot=short&r=json'.format(i))
    df_i=pd.DataFrame(r.json())
    df_origin = df_origin.append(df_i)
```

## Clean data

#### 1. Drop duplicates


```python
df = df_origin.drop_duplicates(subset =['imdbID'], keep='first').reset_index(drop=True)
```

#### 2. Drop columns


```python
df.columns
```




    Index([u'Actors', u'Awards', u'BoxOffice', u'Country', u'DVD', u'Director',
           u'Genre', u'Language', u'Metascore', u'Plot', u'Poster', u'Production',
           u'Rated', u'Ratings', u'Released', u'Response', u'Runtime', u'Title',
           u'Type', u'Website', u'Writer', u'Year', u'imdbID', u'imdbRating',
           u'imdbVotes'],
          dtype='object')




```python
df = df.drop(['Poster','Website','Type','Response','Ratings','Production'],axis=1)
```

#### 3. Replace 'N/A' with NaN


```python
df = df.replace('N/A',np.nan)
```


```python
df = df.dropna(subset=['imdbRating'])
```

#### 4. Clean Country, Genre and Language 


```python
def clean_orders(column_name):
    df[column_name] = df[column_name].apply(lambda x: x.split(', '))
    df[column_name] = df[column_name].apply(lambda x: sorted(x))
    df[column_name] = df[column_name].apply(lambda x: ', '.join(x))
```


```python
clean_orders('Country')
clean_orders('Genre')
```


```python
df['Language'][df['Language'].isnull()] ='English'
```


```python
clean_orders('Language')
```

#### 5. Clean Runtime, Year, imdbRating and imdbVotes


```python
df['Runtime'][df['Runtime'].isnull()] = '91 min'
```


```python
df['Runtime'] = df['Runtime'].apply(lambda x: int(str(x).strip(' min')))
```


```python
df['Year'] = df['Year'].apply(lambda x: int(x))
```


```python
df['imdbRating'] = df['imdbRating'].apply(lambda x: float(x))
```


```python
df['imdbVotes'] = df['imdbVotes'].apply(lambda x: x.replace(',',''))
df['imdbVotes'] = df['imdbVotes'].apply(lambda x: int(x))
```

#### 6. Clean DVD and Released dates


```python
df['DVD'] = pd.to_datetime(df['DVD'])
```


```python
df['Released'] = pd.to_datetime(df['Released'])
```


```python
df['DVD'].fillna(df['Year'],inplace=True)
df['Released'].fillna(df['Year'],inplace=True)
```

#### 7. Clean Rated


```python
df['Rated'] = df['Rated'].apply(lambda x: str(x).replace('NOT RATED','UNRATED'))
df['Rated'] = df['Rated'].apply(lambda x: str(x).replace('PASSED','APPROVED'))
```


```python
df['Rated'] = df['Rated'].apply(lambda x: str(x).replace('nan','UNRATED'))
```

#### 8. Clean Awards


```python
df['Nominations']= df['Awards'].str.extract('([0-9]+ nominations)')
```

    /Users/KatieJi/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)
      if __name__ == '__main__':



```python
df['Nominations'] = df['Nominations'].apply(lambda x: float(str(x).replace(' nominations','')))
```


```python
df['Nominations'] = df['Nominations'].fillna(0)
```


```python
df['Wins']= df['Awards'].str.extract('([0-9]+ wins)')
```

    /Users/KatieJi/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)
      if __name__ == '__main__':



```python
df['Wins'] = df['Wins'].apply(lambda x: float(str(x).replace(' wins','')))
```


```python
df['Wins'] = df['Wins'].fillna(0)
```


```python
df['Oscars']= df['Awards'].str.extract('([0-9]+ Oscars)')
```

    /Users/KatieJi/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)
      if __name__ == '__main__':



```python
df['Oscars'] = df['Oscars'].apply(lambda x: float(str(x).replace(' Oscars','')))
```


```python
df['Oscars'] = df['Oscars'].fillna(0)
```


```python
df = df.drop(['Awards'],axis=1)
```


```python
df['BoxOffice']=df['BoxOffice'].apply(lambda x: str(x).replace('$',''))
df['BoxOffice']=df['BoxOffice'].apply(lambda x: str(x).replace(',',''))
df['BoxOffice']=df['BoxOffice'].apply(lambda x: str(x).replace('.',''))
```


```python
df['BoxOffice']=df['BoxOffice'].apply(lambda x: float(x)/100)
```


```python
df['Metascore']=df['Metascore'].apply(lambda x: float(x))
```


```python
df.head(3)
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Actors</th>
      <th>BoxOffice</th>
      <th>Country</th>
      <th>DVD</th>
      <th>Director</th>
      <th>Genre</th>
      <th>Language</th>
      <th>Metascore</th>
      <th>Plot</th>
      <th>Rated</th>
      <th>...</th>
      <th>Labels</th>
      <th>rated__APPROVED</th>
      <th>rated__G</th>
      <th>rated__GP</th>
      <th>rated__PG</th>
      <th>rated__PG-13</th>
      <th>rated__R</th>
      <th>rated__TV-14</th>
      <th>rated__TV-MA</th>
      <th>rated__UNRATED</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Tim Robbins, Morgan Freeman, Bob Gunton, Willi...</td>
      <td>NaN</td>
      <td>USA</td>
      <td>1970-01-01 00:00:00.000001994</td>
      <td>Frank Darabont</td>
      <td>Crime, Drama</td>
      <td>English</td>
      <td>80.0</td>
      <td>Two imprisoned men bond over a number of years...</td>
      <td>R</td>
      <td>...</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Marlon Brando, Al Pacino, James Caan, Richard ...</td>
      <td>NaN</td>
      <td>USA</td>
      <td>2001-10-09 00:00:00.000000000</td>
      <td>Francis Ford Coppola</td>
      <td>Crime, Drama</td>
      <td>English, Italian, Latin</td>
      <td>100.0</td>
      <td>The aging patriarch of an organized crime dyna...</td>
      <td>R</td>
      <td>...</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Al Pacino, Robert Duvall, Diane Keaton, Robert...</td>
      <td>NaN</td>
      <td>USA</td>
      <td>1970-01-01 00:00:00.000001974</td>
      <td>Francis Ford Coppola</td>
      <td>Crime, Drama</td>
      <td>English, Italian, Latin, Sicilian, Spanish</td>
      <td>80.0</td>
      <td>The early life and career of Vito Corleone in ...</td>
      <td>R</td>
      <td>...</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>3 rows × 32 columns</p>
</div>




```python
df_clean = df
```


```python
df = df_clean
```

## Adding Features

#### 1. Add a new feature Age


```python
df['Age'] = 2017 - df['Year']
```

#### 2. Label y based on imdb ratings


```python
# top 250 movies have ratings greater than 8 - consider ratings greater than 8 as high ratings
df['Labels'] = df['imdbRating'].apply(lambda x: 1 if x >= 8.0 else 0)
```


```python
df_high = df[df['Labels']==1]
df_low = df[df['Labels']==0]
```

## Visualization


```python
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
```

#### 1. Histogram on imdb ratings

From the imdbratings histogram, these movies have a mean of ratings about 7.5.

Most of the ratings are around 6-8.5


```python
fig = plt.figure(figsize=(11,5))
df['imdbRating'].hist()
plt.axvline(df['imdbRating'].mean(),color='b', linestyle='dashed', linewidth=2)
plt.title('imdb rating histogram',fontsize = 16)
```




    <matplotlib.text.Text at 0x111ee2710>




![png]({{ site.baseurl }}/images/output_65_1.png)


#### 2. Heatmap

Age and Year have a very strong relationship because I calculated Age by deducting year from 2017

Wins and Nominations also have a strong relationship, so do Metascore and imdbrating


```python
fig=plt.figure(figsize=(8,6))
sns.heatmap(df.corr()**2)
plt.title('Correlation Heatmap', fontsize=15);
```


![png]({{ site.baseurl }}/images/output_67_0.png)


#### 3. imdbvotes histogram

I plotted imdb votes by type (high score or low score). 

It looks like the low-scored movies are more likely to have fewer votes.

high-scored movies are more widely distributed compared to low-scored movies


```python
df_high.columns
```




    Index([     u'Actors',   u'BoxOffice',     u'Country',         u'DVD',
              u'Director',       u'Genre',    u'Language',   u'Metascore',
                  u'Plot',       u'Rated',    u'Released',     u'Runtime',
                 u'Title',      u'Writer',        u'Year',      u'imdbID',
            u'imdbRating',   u'imdbVotes', u'Nominations',        u'Wins',
                u'Oscars',         u'Age',      u'Labels'],
          dtype='object')




```python
fig = plt.figure(figsize=(16,6))
sns.distplot(df_high['imdbVotes'],color='lightblue',label='high score',bins=5)
sns.distplot(df_low['imdbVotes'],color='lightgreen',label='low score',bins=5)
plt.xlim(0)
plt.title('Imdb Votes Histogram', fontsize=15)
plt.legend();
```


![png]({{ site.baseurl }}/images/output_70_0.png)


#### 4. Year histogram

Low-scored movies are more likely to be recent movies after 2000

High-scored movies are more evenly distributed through 1920 - 2017, while the amount of movies have been increasing through years


```python
fig = plt.figure(figsize=(16,6))
sns.distplot(df_high['Year'],color='lightblue',label='high score',bins=5)
sns.distplot(df_low['Year'],color='lightgreen',label='low score',bins=5)
plt.xlim(1900,2017)
plt.title('Movie Year Histogram', fontsize=15)
plt.legend();
```


![png]({{ site.baseurl }}/images/output_72_0.png)


#### 5. Scatter for Nominations and Wins

From the graph, it looks like Nominations and Wins have a positive linear relationship.

High-scored movies have a higher chance to get above 50 wins and 50 nominations.


```python
mask_high= df_high[df_high['Year']!=2017]
mask_low= df_low[df_low['Year']!=2017]
```


```python
fig = plt.figure(figsize=(12,7))
ax = fig.gca()
plt.scatter(mask_high['Nominations'],mask_high['Wins'],c='lightblue',s=80)
plt.scatter(mask_low['Nominations'],mask_low['Wins'],c='lightgreen',s=80)

ax.set_xlabel('Nominations',fontsize=14)
ax.set_ylabel('Win',fontsize=14)

plt.legend(['high score', 'low score']);
```


![png]({{ site.baseurl }}/images/output_75_0.png)


## Feature engineering

#### 1. Get dummy variables for rated


```python
df= df.join(pd.get_dummies(df['Rated'],prefix='rated_'))
```

#### 2. Train Test split


```python
from sklearn.model_selection import train_test_split
X = df.drop(['imdbRating','imdbID','Labels', 'Rated','DVD','Released'],axis=1)
y = df['Labels']

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state = 42, test_size=.33)
```


```python
X_train.reset_index(drop=True, inplace=True)
X_test.reset_index(drop=True, inplace=True)

y_train.reset_index(drop=True, inplace=True)
y_test.reset_index(drop=True, inplace=True)
```


```python
X_train.head(3)
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Actors</th>
      <th>BoxOffice</th>
      <th>Country</th>
      <th>Director</th>
      <th>Genre</th>
      <th>Language</th>
      <th>Metascore</th>
      <th>Plot</th>
      <th>Runtime</th>
      <th>Title</th>
      <th>...</th>
      <th>Age</th>
      <th>rated__APPROVED</th>
      <th>rated__G</th>
      <th>rated__GP</th>
      <th>rated__PG</th>
      <th>rated__PG-13</th>
      <th>rated__R</th>
      <th>rated__TV-14</th>
      <th>rated__TV-MA</th>
      <th>rated__UNRATED</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Mia Goth, Martin McCann, Andrew Simpson, Barry...</td>
      <td>NaN</td>
      <td>UK</td>
      <td>Stephen Fingleton</td>
      <td>Drama, Sci-Fi, Thriller</td>
      <td>English</td>
      <td>NaN</td>
      <td>In a time of starvation, a survivalist lives o...</td>
      <td>104</td>
      <td>The Survivalist</td>
      <td>...</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Charlie Hunnam, Sienna Miller, Tom Holland, Ro...</td>
      <td>66320.68</td>
      <td>USA</td>
      <td>James Gray</td>
      <td>Action, Adventure, Biography</td>
      <td>English, German, Portuguese, Spanish</td>
      <td>84.0</td>
      <td>A true-life drama, centering on British explor...</td>
      <td>141</td>
      <td>The Lost City of Z</td>
      <td>...</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Charles Chaplin, Paulette Goddard, Henry Bergm...</td>
      <td>NaN</td>
      <td>USA</td>
      <td>Charles Chaplin</td>
      <td>Comedy, Drama, Family</td>
      <td>English</td>
      <td>96.0</td>
      <td>The Tramp struggles to live in modern industri...</td>
      <td>87</td>
      <td>Modern Times</td>
      <td>...</td>
      <td>81</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>3 rows × 26 columns</p>
</div>



#### 3. Create dummy variables using NLP


```python
from sklearn.feature_extraction.text import CountVectorizer
def cvec(df,columns,n_range,max_f):
    cvec = CountVectorizer(ngram_range=n_range,max_features=max_f)
    cvec.fit(df[columns])
    return pd.DataFrame(cvec.transform(df[columns]).todense(),
             columns=cvec.get_feature_names())
```


```python
actors = cvec(X_train,'Actors',(2,3),20)
```


```python
country = cvec(X_train,'Country',(1,1),20)
genre = cvec(X_train,'Genre',(1,1),20)
language = cvec(X_train,'Language',(1,1),20)
writer = cvec(X_train,'Writer',(2,3),20)
title = cvec(X_train,'Title',(1,1),20)
director = cvec(X_train,'Director',(2,3),20)
```


```python
cvec = CountVectorizer(ngram_range=(1,1),max_features=20,stop_words='english')
cvec.fit(X_train['Plot'])
plot = pd.DataFrame(cvec.transform(X_train['Plot']).todense(),
         columns=cvec.get_feature_names())
```


```python
dict_join = {'actors':actors, 'country': country, 'genre': genre, \
             'language': language, 'title': title, 'writer': writer, 'director':director, 'plot': plot}
for k in dict_join:
    X_train = X_train.join(dict_join[k],rsuffix=('_'+k))
```


```python
from sklearn.feature_extraction.text import CountVectorizer
def cvec(df,columns,n_range,max_f):
    cvec = CountVectorizer(ngram_range=n_range,max_features=max_f)
    cvec.fit(df[columns])
    return pd.DataFrame(cvec.transform(df[columns]).todense(),
             columns=cvec.get_feature_names())
```


```python
actors_test = cvec(X_test,'Actors',(2,3),20)
country_test = cvec(X_test,'Country',(1,1),20)
genre_test = cvec(X_test,'Genre',(1,1),20)
language_test = cvec(X_test,'Language',(1,1),20)
writer_test = cvec(X_test,'Writer',(2,3),20)
title_test = cvec(X_test,'Title',(1,1),20)
director_test = cvec(X_test,'Director',(2,3),20)
```


```python
cvec = CountVectorizer(ngram_range=(1,1),max_features=20,stop_words='english')
cvec.fit(X_test['Plot'])
plot_test = pd.DataFrame(cvec.transform(X_test['Plot']).todense(),
         columns=cvec.get_feature_names())
```


```python
dict_join_test = {'actors':actors_test, 'country': country_test, 'genre': genre_test, \
             'language': language_test, 'title': title_test, 'writer': writer_test, \
                  'director':director_test, 'plot': plot_test}
for k in dict_join_test:
    X_test = X_test.join(dict_join_test[k],rsuffix=('_'+k))
```


```python
X_train = X_train.drop(['Actors','Country','Director','Genre','Language','Plot','Title','Writer','Year'],axis=1)
X_test = X_test.drop(['Actors','Country','Director','Genre','Language','Plot','Title','Writer','Year'],axis=1)
```

#### 4. Impute mean for Boxoffice and Metascore


```python
from sklearn.preprocessing import Imputer
 
imputer_m = Imputer(strategy='mean',axis=0).fit(X_train[['Metascore']])
imputer_b = Imputer(strategy='mean',axis=0).fit(X_train[['BoxOffice']])
```


```python
X_train['Metascore'] = imputer_m.transform(X_train[['Metascore']])
X_test['Metascore'] = imputer_m.transform(X_test[['Metascore']])
```


```python
X_train['BoxOffice'] = imputer_b.transform(X_train[['BoxOffice']])
X_test['BoxOffice'] = imputer_b.transform(X_test[['BoxOffice']])
```

#### 5. Scaler


```python
X_train[X_train.isnull().any(axis=1)]
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>BoxOffice</th>
      <th>Metascore</th>
      <th>Runtime</th>
      <th>imdbVotes</th>
      <th>Nominations</th>
      <th>Wins</th>
      <th>Oscars</th>
      <th>Age</th>
      <th>rated__APPROVED</th>
      <th>rated__G</th>
      <th>...</th>
      <th>original story</th>
      <th>original story by</th>
      <th>screen story</th>
      <th>screenplay by</th>
      <th>story by</th>
      <th>story development</th>
      <th>story material</th>
      <th>the book</th>
      <th>the novel</th>
      <th>the novel by</th>
    </tr>
  </thead>
  <tbody>
  </tbody>
</table>
<p>0 rows × 177 columns</p>
</div>




```python
X_test[X_test.isnull().any(axis=1)]
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>BoxOffice</th>
      <th>Metascore</th>
      <th>Runtime</th>
      <th>imdbVotes</th>
      <th>Nominations</th>
      <th>Wins</th>
      <th>Oscars</th>
      <th>Age</th>
      <th>rated__APPROVED</th>
      <th>rated__G</th>
      <th>...</th>
      <th>on the</th>
      <th>on the novel</th>
      <th>original story</th>
      <th>original story by</th>
      <th>screen story</th>
      <th>screenplay christopher</th>
      <th>stanley kubrick_writer</th>
      <th>story by</th>
      <th>story material</th>
      <th>the novel</th>
    </tr>
  </thead>
  <tbody>
  </tbody>
</table>
<p>0 rows × 177 columns</p>
</div>




```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler = scaler.fit(X_train)
X_train_s = scaler.transform(X_train)
X_test_s = scaler.transform(X_test)
```

## Random Forest


```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
```


```python
cv = StratifiedKFold(n_splits=5 , shuffle = True, random_state = 0)

for i in [1,2,3,4,5,None]:
    print 'max depth: {}'.format(i)
    clf = DecisionTreeClassifier(max_depth=i)
    print "DT Score:\t", cross_val_score(clf, X_train_s, y_train, cv=cv, n_jobs=1).mean()
```

    max depth: 1
    DT Score:	0.700420168067
    max depth: 2
    DT Score:	0.763977591036
    max depth: 3
    DT Score:	0.830140056022
    max depth: 4
    DT Score:	0.851484593838
    max depth: 5
    DT Score:	0.849103641457
    max depth: None
    DT Score:	0.825518207283



```python
grid = {
    'n_estimators': [10, 20, 30, 50, 100],
    'max_features': [1,2,3,4,5,6,'auto'],
    'criterion': ['gini','entropy'],
    'class_weight': ["balanced","balanced_subsample",None]
}

cv = StratifiedKFold(n_splits=5 , shuffle = True, random_state = 0)


clf = DecisionTreeClassifier(max_depth=4)
rf = RandomForestClassifier(clf)
gs = GridSearchCV(rf, grid)

model_rf_gs = gs.fit(X_train_s, y_train)
gs.best_params_
```




    {'class_weight': None,
     'criterion': 'entropy',
     'max_features': 'auto',
     'n_estimators': 100}




```python
rf = RandomForestClassifier(max_depth=4, max_features=gs.best_params_['max_features'], n_estimators=gs.best_params_['n_estimators'],\
                            criterion=gs.best_params_['criterion'],class_weight=gs.best_params_['class_weight'])
model_rf = rf.fit(X_train_s, y_train)
```


```python
y_pred_train = model_rf.predict(X_train_s)
y_pred_test = model_rf.predict(X_test_s)

# Confusion matrix on test data
pd.DataFrame(confusion_matrix(y_test,y_pred_test,labels=[1,0]),\
            columns=['predicted_high','predicted_low'], index=['is_high','is_low'])
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>predicted_high</th>
      <th>predicted_low</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>is_high</th>
      <td>87</td>
      <td>4</td>
    </tr>
    <tr>
      <th>is_low</th>
      <td>41</td>
      <td>77</td>
    </tr>
  </tbody>
</table>
</div>




```python
# Accuracy score
print 'accuracy score on training data:', accuracy_score(y_train,y_pred_train)
print 'accuracy score on test data:', accuracy_score(y_test,y_pred_test)
```

    accuracy score on training data: 0.929245283019
    accuracy score on test data: 0.784688995215



```python
# Get features Gini scores
feature_importances = pd.DataFrame(model_rf.feature_importances_, 
                                   index = X_train.columns, columns=['importance'])
feature_importances[feature_importances['importance']!=0].sort_values(by='importance', ascending=False)
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>importance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>imdbVotes</th>
      <td>0.163171</td>
    </tr>
    <tr>
      <th>Metascore</th>
      <td>0.130159</td>
    </tr>
    <tr>
      <th>Age</th>
      <td>0.113337</td>
    </tr>
    <tr>
      <th>Wins</th>
      <td>0.088582</td>
    </tr>
    <tr>
      <th>english</th>
      <td>0.063368</td>
    </tr>
    <tr>
      <th>Oscars</th>
      <td>0.035528</td>
    </tr>
    <tr>
      <th>Runtime</th>
      <td>0.027717</td>
    </tr>
    <tr>
      <th>Nominations</th>
      <td>0.027712</td>
    </tr>
    <tr>
      <th>usa</th>
      <td>0.023858</td>
    </tr>
    <tr>
      <th>BoxOffice</th>
      <td>0.022196</td>
    </tr>
    <tr>
      <th>italian</th>
      <td>0.015489</td>
    </tr>
    <tr>
      <th>rated__PG-13</th>
      <td>0.014189</td>
    </tr>
    <tr>
      <th>italy</th>
      <td>0.013644</td>
    </tr>
    <tr>
      <th>musical</th>
      <td>0.013157</td>
    </tr>
    <tr>
      <th>hindi</th>
      <td>0.010673</td>
    </tr>
    <tr>
      <th>sport</th>
      <td>0.010616</td>
    </tr>
    <tr>
      <th>canada</th>
      <td>0.009466</td>
    </tr>
    <tr>
      <th>alfred hitchcock</th>
      <td>0.008164</td>
    </tr>
    <tr>
      <th>based on</th>
      <td>0.007927</td>
    </tr>
    <tr>
      <th>for</th>
      <td>0.006920</td>
    </tr>
    <tr>
      <th>rated__APPROVED</th>
      <td>0.006887</td>
    </tr>
    <tr>
      <th>horror</th>
      <td>0.006730</td>
    </tr>
    <tr>
      <th>comedy</th>
      <td>0.006597</td>
    </tr>
    <tr>
      <th>based on the</th>
      <td>0.005929</td>
    </tr>
    <tr>
      <th>on the</th>
      <td>0.005812</td>
    </tr>
    <tr>
      <th>rated__UNRATED</th>
      <td>0.005732</td>
    </tr>
    <tr>
      <th>french</th>
      <td>0.005157</td>
    </tr>
    <tr>
      <th>adventure</th>
      <td>0.004849</td>
    </tr>
    <tr>
      <th>war</th>
      <td>0.004746</td>
    </tr>
    <tr>
      <th>harrison ford</th>
      <td>0.004523</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
    </tr>
    <tr>
      <th>new</th>
      <td>0.000495</td>
    </tr>
    <tr>
      <th>ethan coen</th>
      <td>0.000493</td>
    </tr>
    <tr>
      <th>arabic</th>
      <td>0.000490</td>
    </tr>
    <tr>
      <th>joel coen</th>
      <td>0.000485</td>
    </tr>
    <tr>
      <th>man</th>
      <td>0.000478</td>
    </tr>
    <tr>
      <th>created by</th>
      <td>0.000455</td>
    </tr>
    <tr>
      <th>with</th>
      <td>0.000451</td>
    </tr>
    <tr>
      <th>latin</th>
      <td>0.000448</td>
    </tr>
    <tr>
      <th>cantonese</th>
      <td>0.000448</td>
    </tr>
    <tr>
      <th>in</th>
      <td>0.000410</td>
    </tr>
    <tr>
      <th>tom hardy</th>
      <td>0.000399</td>
    </tr>
    <tr>
      <th>sweden</th>
      <td>0.000389</td>
    </tr>
    <tr>
      <th>american</th>
      <td>0.000370</td>
    </tr>
    <tr>
      <th>on</th>
      <td>0.000358</td>
    </tr>
    <tr>
      <th>to</th>
      <td>0.000354</td>
    </tr>
    <tr>
      <th>quentin tarantino</th>
      <td>0.000313</td>
    </tr>
    <tr>
      <th>additional story</th>
      <td>0.000304</td>
    </tr>
    <tr>
      <th>chris pratt</th>
      <td>0.000294</td>
    </tr>
    <tr>
      <th>ben affleck</th>
      <td>0.000273</td>
    </tr>
    <tr>
      <th>life</th>
      <td>0.000243</td>
    </tr>
    <tr>
      <th>street</th>
      <td>0.000196</td>
    </tr>
    <tr>
      <th>story material</th>
      <td>0.000194</td>
    </tr>
    <tr>
      <th>germany</th>
      <td>0.000180</td>
    </tr>
    <tr>
      <th>chinese</th>
      <td>0.000176</td>
    </tr>
    <tr>
      <th>story</th>
      <td>0.000159</td>
    </tr>
    <tr>
      <th>original story</th>
      <td>0.000147</td>
    </tr>
    <tr>
      <th>matthew mcconaughey</th>
      <td>0.000129</td>
    </tr>
    <tr>
      <th>book by</th>
      <td>0.000063</td>
    </tr>
    <tr>
      <th>johnny depp</th>
      <td>0.000059</td>
    </tr>
    <tr>
      <th>francis ford</th>
      <td>0.000014</td>
    </tr>
  </tbody>
</table>
<p>140 rows × 1 columns</p>
</div>



## Extra trees


```python
grid = {
    'n_estimators': [10, 20, 30, 50, 100],
    'max_features': [1,2,3,4,5,6,'auto'],
    'criterion': ['gini','entropy'],
    'class_weight': ["balanced", "balanced_subsample"]
}

cv = StratifiedKFold(n_splits=5 , shuffle = True, random_state = 0)


clf = DecisionTreeClassifier(max_depth=4)
et = ExtraTreesClassifier(clf,n_jobs=1)
gs_es = GridSearchCV(et, grid)

model_et_gs = gs_es.fit(X_train_s, y_train)
gs_es.best_params_
```




    {'class_weight': 'balanced',
     'criterion': 'entropy',
     'max_features': 'auto',
     'n_estimators': 100}




```python
es = ExtraTreesClassifier(max_depth=4, max_features=gs_es.best_params_['max_features'], n_estimators=gs_es.best_params_['n_estimators'],\
                            criterion=gs_es.best_params_['criterion'],class_weight=gs_es.best_params_['class_weight'])
model_es = es.fit(X_train_s, y_train)
```


```python
y_pred_train = model_es.predict(X_train_s)
y_pred_test = model_es.predict(X_test_s)

# Confusion matrix on test data
pd.DataFrame(confusion_matrix(y_test,y_pred_test,labels=[1,0]),\
            columns=['predicted_high','predicted_low'], index=['is_high','is_low'])
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>predicted_high</th>
      <th>predicted_low</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>is_high</th>
      <td>86</td>
      <td>5</td>
    </tr>
    <tr>
      <th>is_low</th>
      <td>53</td>
      <td>65</td>
    </tr>
  </tbody>
</table>
</div>




```python
# Accuracy score
print 'accuracy score on training data:', accuracy_score(y_train,y_pred_train)
print 'accuracy score on test data:', accuracy_score(y_test,y_pred_test)
```

    accuracy score on training data: 0.926886792453
    accuracy score on test data: 0.722488038278



```python
# Get features Gini scores
feature_importances = pd.DataFrame(model_es.feature_importances_, 
                                   index = X_train.columns, columns=['importance'])
feature_importances[feature_importances['importance']!=0].sort_values(by='importance', ascending=False)
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>importance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Age</th>
      <td>0.108544</td>
    </tr>
    <tr>
      <th>Metascore</th>
      <td>0.079075</td>
    </tr>
    <tr>
      <th>usa</th>
      <td>0.074534</td>
    </tr>
    <tr>
      <th>imdbVotes</th>
      <td>0.070593</td>
    </tr>
    <tr>
      <th>Oscars</th>
      <td>0.063962</td>
    </tr>
    <tr>
      <th>english</th>
      <td>0.055273</td>
    </tr>
    <tr>
      <th>rated__PG-13</th>
      <td>0.045749</td>
    </tr>
    <tr>
      <th>musical</th>
      <td>0.036524</td>
    </tr>
    <tr>
      <th>italy</th>
      <td>0.033469</td>
    </tr>
    <tr>
      <th>canada</th>
      <td>0.028664</td>
    </tr>
    <tr>
      <th>Wins</th>
      <td>0.023811</td>
    </tr>
    <tr>
      <th>italian</th>
      <td>0.023613</td>
    </tr>
    <tr>
      <th>hindi</th>
      <td>0.023553</td>
    </tr>
    <tr>
      <th>comedy</th>
      <td>0.021585</td>
    </tr>
    <tr>
      <th>harrison ford</th>
      <td>0.017797</td>
    </tr>
    <tr>
      <th>horror</th>
      <td>0.015362</td>
    </tr>
    <tr>
      <th>alfred hitchcock</th>
      <td>0.014934</td>
    </tr>
    <tr>
      <th>sport</th>
      <td>0.014587</td>
    </tr>
    <tr>
      <th>rated__UNRATED</th>
      <td>0.011067</td>
    </tr>
    <tr>
      <th>based on the</th>
      <td>0.011026</td>
    </tr>
    <tr>
      <th>rated__APPROVED</th>
      <td>0.009953</td>
    </tr>
    <tr>
      <th>christopher nolan</th>
      <td>0.009481</td>
    </tr>
    <tr>
      <th>Nominations</th>
      <td>0.008576</td>
    </tr>
    <tr>
      <th>sergio leone</th>
      <td>0.008012</td>
    </tr>
    <tr>
      <th>based on</th>
      <td>0.007023</td>
    </tr>
    <tr>
      <th>french</th>
      <td>0.006956</td>
    </tr>
    <tr>
      <th>uk</th>
      <td>0.006466</td>
    </tr>
    <tr>
      <th>Runtime</th>
      <td>0.006441</td>
    </tr>
    <tr>
      <th>family_genre</th>
      <td>0.006251</td>
    </tr>
    <tr>
      <th>the</th>
      <td>0.005942</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
    </tr>
    <tr>
      <th>wife</th>
      <td>0.000550</td>
    </tr>
    <tr>
      <th>australia</th>
      <td>0.000543</td>
    </tr>
    <tr>
      <th>clint eastwood</th>
      <td>0.000542</td>
    </tr>
    <tr>
      <th>john huston</th>
      <td>0.000529</td>
    </tr>
    <tr>
      <th>help</th>
      <td>0.000528</td>
    </tr>
    <tr>
      <th>rated__PG</th>
      <td>0.000509</td>
    </tr>
    <tr>
      <th>tom hardy</th>
      <td>0.000473</td>
    </tr>
    <tr>
      <th>wars</th>
      <td>0.000453</td>
    </tr>
    <tr>
      <th>ben affleck</th>
      <td>0.000439</td>
    </tr>
    <tr>
      <th>son</th>
      <td>0.000433</td>
    </tr>
    <tr>
      <th>joel coen</th>
      <td>0.000428</td>
    </tr>
    <tr>
      <th>ewan mcgregor</th>
      <td>0.000413</td>
    </tr>
    <tr>
      <th>star</th>
      <td>0.000402</td>
    </tr>
    <tr>
      <th>life</th>
      <td>0.000381</td>
    </tr>
    <tr>
      <th>john musker</th>
      <td>0.000373</td>
    </tr>
    <tr>
      <th>animation</th>
      <td>0.000358</td>
    </tr>
    <tr>
      <th>back</th>
      <td>0.000355</td>
    </tr>
    <tr>
      <th>the book</th>
      <td>0.000336</td>
    </tr>
    <tr>
      <th>hungarian</th>
      <td>0.000321</td>
    </tr>
    <tr>
      <th>the novel</th>
      <td>0.000255</td>
    </tr>
    <tr>
      <th>cantonese</th>
      <td>0.000243</td>
    </tr>
    <tr>
      <th>ridley scott</th>
      <td>0.000226</td>
    </tr>
    <tr>
      <th>tom hanks</th>
      <td>0.000193</td>
    </tr>
    <tr>
      <th>natalie portman</th>
      <td>0.000192</td>
    </tr>
    <tr>
      <th>tim burton</th>
      <td>0.000191</td>
    </tr>
    <tr>
      <th>thriller</th>
      <td>0.000187</td>
    </tr>
    <tr>
      <th>germany</th>
      <td>0.000154</td>
    </tr>
    <tr>
      <th>gaelic</th>
      <td>0.000135</td>
    </tr>
    <tr>
      <th>biography</th>
      <td>0.000117</td>
    </tr>
    <tr>
      <th>novel by</th>
      <td>0.000035</td>
    </tr>
  </tbody>
</table>
<p>134 rows × 1 columns</p>
</div>


